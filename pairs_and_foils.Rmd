---
title: "Generate GloVe similarities"
output: html_document
date: "2023-11-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(tidyverse)
require(Hmisc)
require(rdist)
require(papaja)
require(stringdist)

elp = read_csv('data/elp_clean.csv') %>% glimpse() %>% 
  select(pair = word, f = Freq_HAL, pos = POS, length = Length) %>% 
  mutate(pair = toupper(pair))

dists = read_csv('data/semantic_distances_among_targets.csv') %>% 
  group_by(target) %>% 
  arrange(distance) %>% 
  mutate(rank = seq_len(n())-1,
         distance_z = (distance-mean(distance))/sd(distance))

dists_missing = read_csv('data/semantic_distances_among_targets_v2.csv') %>% 
  filter(target %in% c("CONDEMN", "MARTYR", "INCARCERATION", "PRODIGAL", "CELLOPHANE", "PESTILENCE")) %>% 
  group_by(target) %>% 
  arrange(distance) %>% 
  mutate(rank = seq_len(n())-1,
         distance_z = (distance-mean(distance))/sd(distance))

targets = read_csv('data/targets.csv', col_names = c('word')) %>% 
  pull(word)


bigwords = read_table('data/glove.6B.50d.txt', col_names = c('word', str_c('d', c(1:50))))
```


### Check to make sure the standardization worked:

```{r}
dists %>% 
  ggplot(aes(distance_z)) +
  geom_histogram(color = 'black') +
  geom_vline(xintercept = 0, linetype = 'dashed', color = 'firebrick4') +
  facet_wrap(vars(target)) +
  labs(x = 'distance') +
  theme_apa() +
  theme(strip.text = element_text(size = 4))

```

## Revised version of stimuli selection
The version of the task the group ultimately settled on requires selecting a single most similar word, and selecting two foils which are farther away. To facilitate this, let's identify the top 200 words from the target, which will allow us to select from the top 10 or so, while also seeing a distribution of words that are farther away.

```{r}
dists %>% 
  filter(rank <= 200) %>% 
  arrange(target) %>% 
  left_join(elp) %>%
  select(target, pair, rank, distance_z, frequency = f, pos) %>% 
  mutate(length_target = str_length(target),
         length_pair = str_length(pair)) %>% glimpse() %>% 
  left_join(read_csv('data/elp_clean.csv') %>% 
              filter(word %in% targets) %>% 
              mutate(word = toupper(word)) %>% 
              select(target = word, frequency_target = Freq_HAL)) %>% 
  select(target, pair, rank, frequency_target, frequency_pair = frequency, length_target, length_pair, distance_z, pos) %>% 
  write_csv('data/semantic_pairs_top_200.csv')

```

### Filter through ELP
Here's a little demo of how to filter through a df (using ELP) for you, Cris.
```{r filterElp}

# get the frequency for subsetting
f = elp %>% 
  filter(word == 'preservative') %>% 
  pull(Freq_HAL)

# filter words that are greater or equal to frequency of "preservative"
# and greater than or equal to length of "preservative" (note AND/& instead of OR/|)
elp %>% 
  filter(Freq_HAL >= f & Length >= str_length('preservative')) %>% 
  select(word, Freq_HAL)


```


### Top 200 for missed items
We missed six items because of formatting issues in the original datafile. These items are: condemn, martyr, incarceration, prodigal, cellophane, and pestilence. We will follow the same routine as above in selecting data for the top 200 pairs for these words.

```{r}
dists_missing %>% 
  filter(rank <= 200) %>% 
  arrange(target) %>% 
  left_join(elp) %>%
  select(target, pair, rank, distance_z, frequency = f, pos) %>% 
  mutate(length_target = str_length(target),
         length_pair = str_length(pair)) %>% glimpse() %>% 
  left_join(read_csv('data/elp_clean.csv') %>% 
              filter(word %in% targets) %>% 
              mutate(word = toupper(word)) %>% 
              select(target = word, frequency_target = Freq_HAL)) %>% 
  select(target, pair, rank, frequency_target, frequency_pair = frequency, length_target, length_pair, distance_z, pos) %>% 
  write_csv('data/semantic_pairs_top_200_missing_words.csv')

```


## Selecting foils
The pairs have been selected through the subsetting and hand-selection process facilitated by the datasets above (`semantic_pairs_top_200.csv` and `semantic_pairs_top_200_missing_words.csv`). 


