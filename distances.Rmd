---
title: "Generate GloVe similarities"
output: html_document
date: "2023-11-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(tidyverse)
require(Hmisc)
require(rdist)
require(papaja)
require(stringdist)

elp = read_csv('data/elp_clean.csv') %>% glimpse() %>% 
  select(pair = word, f = Freq_HAL, pos = POS, length = Length) %>% 
  mutate(pair = toupper(pair))

dists = read_csv('data/semantic_distances_among_targets.csv') %>% 
  group_by(target) %>% 
  arrange(distance) %>% 
  mutate(rank = seq_len(n())-1,
         distance_z = (distance-mean(distance))/sd(distance))

targets = readxl::read_xlsx('data/Final_60_new_2.xlsx') %>% 
  mutate(word = tolower(Word)) %>% 
  pull(word)


bigwords = read_table('data/glove.6B.50d.txt', col_names = c('word', str_c('d', c(1:50))))

```


### Check to make sure the standardization worked:

```{r}
dists %>% 
  ggplot(aes(distance_z)) +
  geom_histogram(color = 'black') +
  geom_vline(xintercept = 0, linetype = 'dashed', color = 'firebrick4') +
  facet_wrap(vars(target)) +
  labs(x = 'distance') +
  theme_apa() +
  theme(strip.text = element_text(size = 4))

```

## Routine for selecting near and far semantic pairs:
### Near pair
The most semantically similar word will always be the word with rank 1, where the word with rank == 0 with be the target itself (i.e., the most similar word semantically will be itself). This sometimes yields a good nearest neighbor (ZUCCHINI) but sometimes doesn't (SYNAGOGUE, or SINCERITY).

```{r}
dists %>% 
  filter(rank == 1)
  
```

We will narrow the search space, looking around rank 1 (1-10), around rank 50 (45-60), and around rank 1000 (995-1010). We will also merge in frequency data and calculate length to try to control for those factors (as much as possible).

```{r}
dists %>% 
  filter(rank %in% c(1:10, 45:60, 995:1010)) %>% 
  arrange(target) %>% 
  left_join(elp) %>%
  select(target, pair, rank, distance_z, frequency = f, pos) %>% 
  mutate(length_target = str_length(target),
         lenth_pair = str_length(pair)) %>% glimpse() %>% 
  left_join(read_csv('data/elp_clean.csv') %>% 
              filter(word %in% targets) %>% 
              mutate(word = toupper(word)) %>% 
              select(target = word, frequency_target = Freq_HAL)) %>% 
  write_csv('data/semantic_pair_contenders.csv')

```

## Mid neighbor

```{r}

dists %>% 
  filter(rank == 10) %>% 
  select(target, pair, distance) %>% 
  knitr::kable()

```

### Far pair
Here, we can take the word that is closest to zero.
```{r}
dists %>% 
  group_by(target) %>% 
  mutate(distance_absolute = abs(distance_z)) %>% 
  arrange(distance_absolute) %>% 
  mutate(rank_absolute = seq_len(n())) %>% 
  ungroup() %>% 
  filter(rank_absolute == 1) %>% 
  select(target, pair, distance) %>% 
  knitr::kable()
```


## Very far

```{r}
dists %>% 
  group_by(target) %>% 
  arrange(desc(distance)) %>% 
  mutate(rank_reverse = seq_len(n())) %>% 
  filter(rank_reverse == 10) %>% 
  select(target, pair, distance) %>% 
  knitr::kable()
```


The new version of the task requires selecting a single most similar among foils, all of whom are farther away. For this, let's identify the top 200 words from the target, which will allow us to select from the top 10 or so, while also seeing a distribution of words that are farther away.

```{r}
dists %>% 
  filter(rank <= 200) %>% 
  arrange(target) %>% 
  left_join(elp) %>%
  select(target, pair, rank, distance_z, frequency = f, pos) %>% 
  mutate(length_target = str_length(target),
         length_pair = str_length(pair)) %>% glimpse() %>% 
  left_join(read_csv('data/elp_clean.csv') %>% 
              filter(word %in% targets) %>% 
              mutate(word = toupper(word)) %>% 
              select(target = word, frequency_target = Freq_HAL)) %>% 
  select(target, pair, rank, frequency_target, frequency_pair = frequency, length_target, length_pair, distance_z, pos) %>% 
  write_csv('data/semantic_pairs_top_200.csv')

```

## Filter through ELP
Here's a litte demo of how to filter through a df (using ELP).
```{r filterElp}

# get the frequency for subsetting
f = elp %>% 
  filter(word == 'preservative') %>% 
  pull(Freq_HAL)

# filter words that are greater or equal to frequency of "preservative"
# and greater than or equal to length of "preservative" (note AND/& instead of OR/|)
elp %>% 
  filter(Freq_HAL >= f & Length >= str_length('preservative')) %>% 
  select(word, Freq_HAL)


```











